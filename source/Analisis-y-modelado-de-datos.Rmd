---
title: "Análisis y modelado de datos"
author: "Pablo Domínguez"
date: '2022-06-23'
output: pdf_document
bibliography: references.bib  
header-includes:
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{xcolor}
---

```{r message=FALSE, echo=FALSE}
library(tidyverse)
library(kableExtra)
library(ggpubr)
library(randomForest)
library(mlbench)
library(caret)
library(FactoMineR)
library(factoextra)
library(mltools)
library(data.table)
```

## Planteamiento del problema a abordar

Nos encontramos con un conjunto de datos obtenidos a partir de mediciones meteorológicas realizadas por el gobierno de Australia^[Notes about Daily Weather Observations - @NADWO]. Estos datos, recogidos en distintas localidades, se han capturado realizando mediciones diarias de temperatura, lluvia, evaporación, sol, viento, humedad etc. 

En la referencia mencionada advierten que el control de calidad aplicado a la captura de estos datos ha sido limitado, por lo que es posible que existan imprecisiones debidas a datos faltantes, valores acumulados tras varios datos faltantes o errores de varios tipos. Es por este motivo que empezaremos nuestro estudio realizando una revisión de la calidad y estructura del dato. Tras este proceso, construiremos una serie de variables que transformarán el problema y la estructura de datos para que puedan aplicarse los modelos de clasificación supervisada planteados.

Partiendo de la base de datos procesada, la segmentaremos para aplicar varios modelados diferentes por zonas (siguiendo cierto criterio). Finalmente, compararemos los modelos, los ensamblaremos y presentaremos unos resultados de la precisión del modelo final.

Con esta aplicación práctica de los modelos teóricos abordados en el capítulo anterior buscamos reflejar la capacidad de herramientas matemáticas abstractas a la hora de resolver situaciones que pueden tener un gran beneficio en varios ámbitos, tales como sociales, económicos o medioambientales.

## Origen de los datos y variable objetivo

El buró de metereología australiano coordina una serie de estaciones metereológicas locales repartidas a lo largo del territorio. De esta manera, recopila y reporta datos sobre mediciones meteorológicas. En nuestro caso, tenemos información de **(numero)** ciudades repartidos a lo largo de **cantidad** años.

```{r echo=FALSE}

x <- "rep(\"---\", 6)"
xnew <- "---"

db <- read.csv("../db/weatherAUS.csv", stringsAsFactors = TRUE)
db$Date <- as.Date(db$Date, format="%Y-%m-%d")
attach(db)
db %>% head() %>%  select(-c(6:19)) %>% add_column(.,rep("---",6),.after = 5) %>% rename(., !!xnew := !!rlang::sym(x)) %>% kbl(., booktabs = T,caption = "Muestra de los datos[note]",) %>% kable_styling(latex_options = c("striped", "scale_down")) 

# Voy a descomponer en:
# - trend
# - seasonality
# - noise
# 
# SMA(n):moving average of last n days --> smoothing
# decompose()
```

```{r Tipos de datos}
# Buscar manera alternativa (representable) de evaluar los tipos de dato.
str(db)
```


## Información de los datos

```{r Rangos de fechas, echo=FALSE}

# Creamos función que comprueba rango de fechas

err_cities <- c()
date_range <- function(){
  # Inicializamos los vectores
  min_fec <- c()
  max_fec <- c()
  obs <- c()
  rep <- c()
  n_diff_dates_vec <- c()
  top_diff_date_vec <- c()
  range_free_vec <- c()
  
  # Iteramos por cada ciudad
  for(city in levels(db$Location)){
    
    # Filtramos el ds por ciudad
    db_filtered <- db %>% filter(.,Location==city)
    
    # Calculamos y almacenamos el mínimo y el máximo de la variable Date
    mind <- format(as.Date(min(db_filtered$Date),format="%Y-%m-%d"))
    min_fec <- c(min_fec,mind)
    maxd <- format(as.Date(max(db_filtered$Date),format="%Y-%m-%d"))
    max_fec <- c(max_fec,maxd)
    
    # Comprobamos rangos de fecha
    fech_range <- seq(mind %>% as.Date(), maxd %>% as.Date(), "days")
    diff_dates <- setdiff(fech_range,db_filtered$Date) %>% as.Date(., origin="1970-01-01")
    if(length(diff_dates)>0){ # hay fechas faltantes
      top_diff_date <- max(diff_dates) %>% as.Date(., origin="1970-01-01")
      top_diff_date_vec <- c(top_diff_date_vec,top_diff_date)
      n_diff_dates <- length(diff_dates)
      n_diff_dates_vec <- c(n_diff_dates_vec,n_diff_dates)
      range_free <- seq(top_diff_date %>% as.Date(),maxd %>% as.Date(), "days") %>% length()
      range_free_vec <- c(range_free_vec,range_free)
    } else {
      top_diff_date_vec <- c(top_diff_date_vec,NA)
      n_diff_dates_vec <- c(n_diff_dates_vec,0)
      range_free_vec <- c(range_free_vec,NA)
    }
    obs <- c(obs,length(db_filtered$Date))
    rep <- c(rep,length(unique(db_filtered$Date))!=length(db_filtered$Date))
  }
  
  dates_df <- data.frame(min_fec,max_fec,obs,rep,n_diff_dates_vec,top_diff_date_vec,range_free_vec, stringsAsFactors=TRUE)
  dates_df$min_fec <- as.Date(dates_df$min_fec, format="%Y-%m-%d")
  dates_df$max_fec <- as.Date(dates_df$max_fec, format="%Y-%m-%d")
  dates_df <- mutate(dates_df, range = max_fec - min_fec+1)
  dates_df$top_diff_date_vec <- dates_df$top_diff_date_vec %>% as.Date(., origin="1970-01-01")
  
  rownames(dates_df) <- levels(db$Location)
  dates_df <- dates_df %>% rename(n_diff_dates=n_diff_dates_vec, top_diff_date=top_diff_date_vec, range_free = range_free_vec)
  return(dates_df)
}
dates_df <- date_range()
dates_df %>% View()

```

*renombrar columnas
Hablamos sobre la brecha de fechas, sobre los datos faltantes y el "salto" cuando ya no hay datos faltantes: 2013-03-01. Filtramos por el último valor a partir del cual no hay datos faltantes, esto es, 2013-12-31

```{r Filtramos por fecha mayor a 2013-12-31 y menor a 2017-06-24}

# Rango de a 2013-12-31 y menor a 2017-06-24
db <- db %>% filter(.,Date>as.Date("2013-12-31") & Date<=as.Date("2017-06-24"))

# Comprobamos datos limpios
dates_df <- date_range()
dates_df %>% View()

```

Hablar de cómo hemos asignado las zonas climáticas. Puto google maps y el mapa ese del aus gov


```{r Agrupamos por zonas climáticas}

# Creamos vectores de zonas climáticas
zona1 <- c("Exmouth", "Dampier", "PortHedland", "Broome", "Derby", "Wyndham", "TimberCreek", "Katherine","Darwin", "Oenpelli", "Borroloola", "Nhulunbuy","Burketown", "Weipa", "Cooktown", "Cairns", "Townsville")
zona2 <- c("Mackay", "Rockhampton", "Maryborough", "Brisbane", "CoffsHarbour","GoldCoast")
zona3 <- c("Goondiwindi", "Taroom","Charleville", "Longreach","Thargomindah","Birdsville","MountIsa","AliceSprings","Kulgera","Yulara","Telfer","Newman", "GascoyneJunction","Carnavon","Uluru")
zona4 <- c("Woomera","Yalgoo", "Wiluna", "KalgoorlieBoulder", "Norseman", "Merredin","Newdegate","Warburton", "Amata","Oodnadatta","CooberPedy", "Nullarbor", "Innamincka","Whyalla","BrokenHill","Tibooburra","Bourke","Ivanhoe","Mildura","Griffith","Albury","WaggaWagga","Wodonga","Dubbo","Tamworth","Shepparton","Cobar","Moree","Nhil")
zona5 <- c("Geraldton","Perth","Witchcliffe","Bunbury","MargaretRiver","Esperance","Eucla","Ceduna","PortLincoln","Adelaide","LeighCreek","Renmark","Wollongong","Sydney","SydneyAirport","Newcastle","PortMacquaire","NorahHead","PearceRAAF","Penrith","PerthAirport")
zona6 <- c("Albany","Burra","Kingscote","KingstonSE","MountGambier","Horsham","Watsonia","Melbourne","LakesEntrance","BadgerysCreek","Bendigo","Dartmoor","MelbourneAirport","Nuriootpa","Sale","SalmonGums")
zona7 <- c("Ballarat","Canberra","Bathurst","Devonport","Strahan","Launceston","Swansea","Hobart","Southport","Tuggeranong")
zona8 <- c("MountGinini")

zonas <- c(zona1,zona2,zona3,zona4,zona5,zona6,zona7,zona8)
zona_climatica <- c()

# Asignamos cada ciudad a una zona climática
for(city in levels(db$Location)){
  if(city %in% zona1){
    zona_climatica <- c(zona_climatica,1)
  }
  else if(city %in% zona2){
    zona_climatica <- c(zona_climatica,2)
  }
  else if(city %in% zona3){
    zona_climatica <- c(zona_climatica,3)
  }
  else if(city %in% zona4){
    zona_climatica <- c(zona_climatica,4)
  }
  else if(city %in% zona5){
    zona_climatica <- c(zona_climatica,5)
  }
  else if(city %in% zona6){
    zona_climatica <- c(zona_climatica,6)
  }
  else if(city %in% zona7){
    zona_climatica <- c(zona_climatica,7)
  } 
  else if(city %in% zona8){
    zona_climatica <- c(zona_climatica,8)
  } 
  else { # Los lugares sin zona climática le asignamos el 0, para luego eliminar estas observaciones
    zona_climatica <- c(zona_climatica,0)
  }
}

# Creamos dataframe de zonas climáticas
zonas_climaticas <- data.frame(levels(db$Location),zona_climatica) %>% rename(Location=levels.db.Location.)
zonas_climaticas %>% View()

```

Hablar de otros posibles criterios con los que segmentar los datos. Clustering no supervisado con k=8 y comparar el ratio de coincidencia de una categorización contra la otra.

Hemos eliminado localizaciones las cuales no ha sido posible determinar la zona climática a la que pertenecen. En concreto, se han eliminado *NorfolkIsland*, *Portland*, *Richmond*, *Walpole* y *Williamtown*. Mostramos observaciones por zona climática.

```{r Información por zona climática}

# Añadimos las zonas climáticas

db <- left_join(zonas_climaticas,db, by="Location",strings)
db$Location <- db$Location %>% as.factor()

# Eliminamos la zona climática 0
db <- db %>% filter(., zona_climatica != 0)
db <- droplevels(db)
#db$zona_climatica <- db$zona_climatica %>% as.factor()

# Contamos valores totales por zona climática:
db %>% count(zona_climatica) %>% View()

# Contamos cantidad de ciudades por zona climática
db %>% count(zona_climatica,Location) %>%count(zona_climatica,name="n_locations") %>%  View()

# Comprobamos datos continuos
db %>% count(zona_climatica,Location) %>% View() # esto confirma que tenemos 1272 datos para cada ciudad
dates_df <- date_range()
dates_df %>% View()


```

Comprobar tipos de datos

## División y limpiado de los datos

```{r}

db_zone1 <- db %>% filter(., zona_climatica == 1) %>% select(., -zona_climatica)
db_zone1 <- droplevels(db_zone1)
db_zone2 <- db %>% filter(., zona_climatica == 2) %>% select(., -zona_climatica)
db_zone2 <- droplevels(db_zone2)
db_zone3 <- db %>% filter(., zona_climatica == 3) %>% select(., -zona_climatica)
db_zone3 <- droplevels(db_zone3)
db_zone4 <- db %>% filter(., zona_climatica == 4) %>% select(., -zona_climatica)
db_zone4 <- droplevels(db_zone4)
db_zone5 <- db %>% filter(., zona_climatica == 5) %>% select(., -zona_climatica)
db_zone5 <- droplevels(db_zone5)
db_zone6 <- db %>% filter(., zona_climatica == 6) %>% select(., -zona_climatica)
db_zone6 <- droplevels(db_zone6)
db_zone7 <- db %>% filter(., zona_climatica == 7) %>% select(., -zona_climatica)
db_zone7 <- droplevels(db_zone7)
db_zone8 <- db %>% filter(., zona_climatica == 8) %>% select(., -zona_climatica)
db_zone8 <- droplevels(db_zone8)
# Creamos diccionario con los df para poder iterar
zonas <- list("zona1"=db_zone1,
           "zona2"=db_zone2,
           "zona3"=db_zone3,
           "zona4"=db_zone4,
           "zona5"=db_zone5,
           "zona6"=db_zone6,
           "zona7"=db_zone7,
           "zona8"=db_zone8)
print(nrow(db) == (nrow(db_zone1) + nrow(db_zone2) + nrow(db_zone3) + nrow(db_zone4) + nrow(db_zone5) + nrow(db_zone6) + nrow(db_zone7) + nrow(db_zone8)))


```

### Tratamiento de los atributos: missing, outliers, normalización, balanceo de datos--> Renombrar: Calidad del dato.

Hablar que tratamos los datos por cada zona, de forma que la imputación de valores faltantes respete dicha división.

#### Missings

```{r Detect missings}

# Crear función de comprobar missings
# counting missing values
get_missings <- function(){
  df_missings <- NULL;
  total_rows <- c()
  for(i in 1:length(zonas)){
    row <- zonas[[i]] %>% select(everything()) %>% summarise_all(funs(sum(is.na(.))))*100 / nrow(zonas[[i]])
    total_rows <- c(total_rows,nrow(zonas[[i]]))
    df_missings <- df_missings %>% rbind(.,row)
  }
  df_missings["Total obs"] <- total_rows
  rownames(df_missings) <- c("zona1",
                              "zona2",
                              "zona3",
                              "zona4",
                              "zona5",
                              "zona6",
                              "zona7",
                              "zona8")
  return(df_missings)
  
}
df_missings <- get_missings()
df_missings %>% View()

# Vamos a eliminar todas las variables que tienen el 100% de NA en la zona8, esto es,
# Evaporation, Sunshine, Pressure9am, Pressure3pm, Cloud9am, Cloud3pm, ya que hay indicativos de que esta información ha sido grabada incorrectamente (con un alto número de NAs para todas las zonas climáticas).

```

```{r}
for(i in 1:length(zonas)){
  zonas[[i]] <- zonas[[i]] %>% select(., -any_of(c("Evaporation", "Sunshine", "Pressure9am", "Pressure3pm", "Cloud9am", "Cloud3pm")))
}
```


Metrics imputations is a way to fill NaN values with some special metrics that depend on your data: mean or median for example.

Mean value is the sum of a value in a series divided by a number of all values of series. It is one of the most used types of metrics in statistics. But why do we impute the NaN values with mean value? Mean has a very interesting property, it doesn’t change if you add some more mean values to your series.

```{r}
calc_mode <- function(x){
  
  # List the distinct / unique values
  distinct_values <- unique(x)
  
  # Count the occurrence of each distinct value
  distinct_tabulate <- tabulate(match(x, distinct_values))
  top <- which.max(distinct_tabulate) 
  # Return the value with the highest occurrence
  mode <- distinct_values[top]
  if(is.na(mode)){
    top <- distinct_tabulate[distinct_tabulate!=distinct_tabulate[top]] %>% which.max()
    mode <- distinct_values[top]
  }
  return(mode)
}
```


```{r Replace missings}
# mutate missing values

columnas_enteras <- zonas[[1]][, unlist(lapply(zonas[[1]], is.integer), use.names = FALSE) ] %>% colnames()
columnas_numericas <- zonas[[1]][, unlist(lapply(zonas[[1]], is.numeric), use.names = FALSE) ] %>% colnames() %>% setdiff(.,columnas_enteras)
columnas_categoricas <- zonas[[1]][, unlist(lapply(zonas[[1]], is.factor), use.names = FALSE) ] %>% colnames()

# reemplazamos variables continuas por la media
for(i in 1:length(zonas)){
  zonas[[i]] <- zonas[[i]] %>% mutate_at(columnas_numericas, ~replace_na(.,mean(., na.rm = TRUE))) 
}

# reemplazamos variables enteras por la media truncada
for(i in 1:length(zonas)){
  zonas[[i]] <- zonas[[i]] %>% mutate_at(columnas_enteras, ~replace_na(.,floor(mean(., na.rm = TRUE)))) 
}

# reemplazamos variables categóricas por la moda
for(i in 1:length(zonas)){
  zonas[[i]] <- zonas[[i]] %>% mutate_at(columnas_categoricas, ~replace_na(.,calc_mode(.))) 
}

df_missings <- get_missings()
df_missings %>% View()

```

#### Outliers

```{r Get outliers, eval = FALSE}

get_outliers <- function(){
  plots_list <- list()
  for(i in 1:length(zonas)){
    ps <- list()
    db_temp <- zonas[[i]][,c(columnas_enteras,columnas_numericas)]
    for(colu in db_temp %>% colnames() %>% setdiff(.,"RainTomorrow")){
      p <- ggplot(zonas[[i]], aes_string(x="RainTomorrow", y=colu, color="RainTomorrow")) + geom_violin() + geom_boxplot(width=0.25) + stat_boxplot(geom = "errorbar", width = 0.2) + theme(
        axis.text.x = element_blank(),
        axis.text.y = element_text(size=6),
        axis.title.x = element_text(size = 8),
        axis.title.y = element_text(size = 8),
        legend.key.size = unit(0.1, 'cm'),
        legend.text = element_text(size=8),
        legend.title = element_blank())
      ps[[colu]] <- p
    }
    new_name <- paste0("zona",as.character(i))
    plots_list[[new_name]] <- ggarrange(plotlist =  ps, nrow = 4,ncol = 3,common.legend = TRUE)
    dev.off()
  }
  return(plots_list)
}

plot_list <- get_outliers()

# For zona in names(plot_list) print nicely test["zona1"]$zona1
```

No vamos a tratar los outliers porque consideramos que son de valor para el entrenamiento de los modelos. Nos limitaremos entonces a realizar una normalización de los datos numéricos junto con una selección de variables aplicando por PCA y FAMD

#### Transformación de las variables categóricas

Las variables categóricas que disponemos son "Location"     "WindGustDir"  "WindDir9am"   "WindDir3pm"   "RainToday"    "RainTomorrow". Para la 1,5 y 6 vamos a aplicar técnicas de one hot encoding. Por otra parte, para las relativas a la dirección del viento, vamos a aplicar una transformación de forma que combinemos esta información con las variables enteras relativas a la intensidad del viento, obteniendo una información numérica que expresa el valor del coseno y del seno de dicha dirección.


```{r Transformación de variables categóricas}

# Iterar por zonas
dmy <- dummyVars( ~ +Location+RainToday+RainTomorrow, data = zonas[[1]])
trsf <- data.frame(predict(dmy, newdata = zonas[[1]]))
zonas[[1]] <- zonas[[1]] %>% select(., -any_of(c("Location","RainToday","RainTomorrow"))) %>% cbind(.,trsf)
zonas[[1]] %>% head() %>% View()
```

```{r Transformación de variables de viento}


radianes <- list("E"=0,"ENE"=pi/8, "NE"=pi/4, "NNE"=3*pi/8, 
                 "N" = pi/2,"NNW"=5*pi/8,"NW"=3*pi/4,"WNW"=7*pi/8,
                 "W"=pi,"WSW"=9*pi/8,"SW"=5*pi/4,"SSW"=11*pi/8,
                 "S"=3*pi/2,"SSE"=13*pi/8,"SE"=7*pi/4,"ESE"=15*pi/8)

zonas[[1]] %>% transform(., WindGustDir=radianes[as.character(zonas[[1]]$WindGustDir)]) %>% head() %>% View()




```





#### Normalización de datos.

justificar la normalización. Bien para el modelado, bien para el FS

## Eliminación de variables y representación de los datos

### Feature selection

RESUMEN DE TODO: 
- Las columnas categóricas son:"Location"     "WindGustDir"  "WindDir9am"   "WindDir3pm"   "RainToday"    "RainTomorrow". Vamos a sustituir las Wind_cosas por dos variables cada una, que indican el valor del seno (componente norte-sur) y el coseno(componente este-oeste) de la dirección del viento. Además, como tenemos información de la intesidad del viento por hora, multiplicaremos estos valores para agregar la información. Con este criterio, transformamos variablas variables numéricas y categóricas, las cuales con el resto de variables numéricas someteremos a un algoritmo step para seleccionar las más relevantes para el modelado. Adicionalmente, nos restan las variables categóricas "Location" y "RainToday", las cuales someteremos a un onehot encoding para el modelado.



**Con las numéricas normalizamos y hacemos el step* y con las categóricas hacemos PCA

Factorial Analysis of Mixed Data (FAMD)

As presented in^[An Introduction to Variable and Feature Selection - @AITFS], hemos seguido los pasos que proponen a la hora de trabajar, es decir:

 - Cosas de la lista
 
```{r step for selecting most relevant numeric variables, eval=FALSE}

summary(lm1 <- lm(Fertility ~ ., data = swiss))
slm1 <- step(lm1)
summary(slm1)
slm1$anova

```



### Representación

Distribuciones (histogramas)

Comprobar la proporción de dias que llueve vs dias que no por zona (rain_ratio)

## Creación de nuevas variables ventana para trend y seasonality

Crear ventanas para RainToday de estadísticos rolling patrá

## Modelado

### Train-test split