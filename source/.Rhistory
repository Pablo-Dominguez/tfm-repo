library(readxl)
library(dplyr)
library(knitr)
library(ggplot2)
library(ggpubr)
library(pastecs)
library(kableExtra)
library (e1071)
library(lemon)
# Listado de modelos tuneados guardados
saved_models <- list.files("../models/tuned/")
# Modelado con kernel lineal
if("tuned_linmod02.Rds" %in% saved_models){
tuned_linmod02 <- readRDS("../models/tuned/tuned_linmod02.Rds")
print("Modelado con kernel lineal")
tuned_linmod02_sum <- summary(tuned_linmod02)
tuned_linmod02_sum %>% print()
print(paste0("Error promedio: ",tuned_linmod02_sum$performances$error %>% mean(),
". Desv. típica del error: ", tuned_linmod02_sum$performances$error %>% sd()))
} else {
tuned_linmod02 <- tune(svm ,Class ~ ., data = train, kernel = "linear",
ranges =list(cost=seq(from = 1, to = 10, by=0.25) ),
tunecontrol = tune.control(cross=10))
saveRDS(tuned_linmod02,"../models/tuned/tuned_linmod02.Rds")
tuned_linmod01 <- readRDS("../models/tuned/tuned_linmod02.Rds")
print("Modelado con kernel lineal")
tuned_linmod02_sum <- summary(tuned_linmod02)
tuned_linmod02_sum %>% print()
print(paste0("Error promedio: ",tuned_linmod02_sum$performances$error %>% mean(),
". Desv. típica del error: ", tuned_linmod02_sum$performances$error %>% sd()))
}
# Modelado refinado con kernel radial
if("tuned_radmod03.Rds" %in% saved_models){
tuned_radmod03 <- readRDS("../models/tuned/tuned_radmod03.Rds")
print("Modelado refinado con kernel radial")
tuned_radmod03_sum <- summary(tuned_radmod03)
print(tuned_radmod03_sum)
print(paste0("Error promedio: ",tuned_radmod03_sum$performances$error %>% mean(),
". Desv. típica del error: ", tuned_radmod03_sum$performances$error %>% sd()))
} else {
tuned_radmod03 <- tune(svm ,Class ~ ., data = st_train, kernel = "radial",
ranges =list(cost=2**seq(from=-5, to=10),
gamma=2**seq(from=-5, to=10)),
tunecontrol = tune.control(cross=5))
saveRDS(tuned_radmod03,"../models/tuned/tuned_radmod03.Rds")
tuned_radmod03 <- readRDS("../models/tuned/tuned_radmod03.Rds")
print("Modelado refinado con kernel radial")
tuned_radmod03_sum <- summary(tuned_radmod03)
print(tuned_radmod03_sum)
print(paste0("Error promedio: ",tuned_radmod03_sum$performances$error %>% mean(),
". Desv. típica del error: ", tuned_radmod03_sum$performances$error %>% sd()))
}
tuned_radmod03
svmfit <- tuned_radmod03$best.model
plot(svmfit , st_train)
saved_dbs <- list.files("../dbs/model_db/")
if("df_final.Rds" %in% saved_dbs){
df_final <- readRDS(file="../dbs/model_db/df_final.Rds")
} else {
df_final$Class <- as.factor(df_final$Class)
saveRDS(df_final, file="../dbs/model_db/df_final.Rds")
}
# Creamos los conjuntos de entrenamiento y test
df_final$id <- 1:nrow(df_final)
set.seed(210)
train <- df_final %>% dplyr::sample_frac(.75)
test  <- dplyr::anti_join(df_final, train, by = 'id')
# Eliminamos la columna id
train <- train[,!(names(train) %in% c("id"))]
test <- test[,!(names(test) %in% c("id"))]
# Estandarizamos el conjunto de entrenamiento.
st_train <- scale(train[,1:(ncol(train)-1)],center = TRUE, scale = TRUE) %>% as.data.frame()
st_train$Class <- train$Class # Espero estar incluyendo la clase en el mismo orden. Revisar.
# Calculamos el vector de medias y el vector de desviaciones estándar del conjunto train.
means <- train %>% summarise_if(is.numeric, mean)
st.devs <- train %>% summarise_if(is.numeric, sd)
# Restamos las medias y dividimos por las desviaciones estándar
st_test <- scale(test[,1:(ncol(test)-1)],center = means, scale = st.devs) %>% as.data.frame()
st_test$Class <- test$Class
plot(svmfit , st_train)
plot(svmfit$ , st_train, Class~.)
plot(svmfit , st_train, Class~.)
plot(svmfit , data=st_train)
plot(svmfit , st_train, Class~.)
st_train %>% sapply(class)
st_train %>%  mutate_if(is.factor, funs(as.numeric(as.character(.))))
st_train$Class
st_train$Class %>% as.numeric()
st_train$Class %>% as.character() %>% as.numeric()
st_train$Class <- st_train$Class %>% as.numeric()
plot(svmfit , st_train, Class~.)
plot(svmfit , st_train, Class~Area + Perimeter)
plot(svmfit , st_train, st_train$Class~st_train$Area + st_train$Perimeter)
plot(svmfit , st_train, st_train$Class~st_train$Area + st_train$Perimeter+st_train$Major_Axis_Length+st_train$Minor_Axis_Length+st_train$Convex_Area)
tuned_radmod03$best.parameters
svmfit <-  svm(st_train$Class~., data=st_train, kernel ="radial",gamma =0.03125, cost=64)
plot(svmfit , st_train)
plot(svmfit , st_train)
plot(svmfit , st_train, st_train$Class~st_train$Area + st_train$Perimeter)
tuned_radmod03$best.parameters
tuned_radmod03$best.parameters$gamma
st_train$Class <- st_train$Class %>% as.numeric()
svmfit <-  svm(st_train$Class~., data=st_train,
kernel ="radial",
gamma =tuned_radmod03$best.parameters$gamma,
cost=tuned_radmod03$best.parameters$cost)
plot(svmfit , st_train)
plot(svmfit , st_train)
plot(svmfit , st_train)
p <- plot(svmfit , st_train)
print(p)
p <- plot(svmfit , st_train, formula = st_train$Class~.)
print(p)
?plot.svm
data(st_train)
st_train %>% data("bnr")
?data
p <- plot(svmfit , st_train, formula = Area~Perimeter)
p
p <- plot(svmfit , st_train, formula = st_train$Area~st_train$Perimeter)
p
library(readr)
car <- read.csv("~/Desktop/RTG/dataset/car.data.txt", header = F)
# V7: unacc, acc, good, vgood
roww <- nrow(car)
coll <- ncol(car)
numTrain <- floor((2/3) * roww)
numTest <- roww - numTrain
training <- car[sample(roww, numTrain), ]
library(readr)
cars
library(tidyverse)
library(kableExtra)
library(ggpubr)
library(randomForest)
library(mlbench)
library(caret)
library(FactoMineR)
library(factoextra)
library(mltools)
library(data.table)
?knn
1. Definir un conjunto de datos sobre los que modelar el problema de clasificación.
2. Seleccionar un conjunto de \textit{variables predictoras} relevante para el problema.
3. Tratar incorrecciones o valores faltantes
4. Estandarizar variables
5. Creación de nuevas variables.
6. Modelado del problema de clasificación con diferentes algoritmos.
7. Presentar, visualizar, comparar e interpretar los resultados.
8. Conclusiones
?str
setwd("~/Documents/MUM/TFM/code/source")
x <- "rep(\"---\", 6)"
xnew <- "---"
db <- read.csv("../db/weatherAUS.csv", stringsAsFactors = TRUE)
db$Date <- as.Date(db$Date, format="%Y-%m-%d")
attach(db)
db %>% head() %>%  select(-c(6:19)) %>% add_column(.,rep("---",6),.after = 5) %>% rename(., !!xnew := !!rlang::sym(x)) %>% kbl(., booktabs = T,caption = "Muestra de algunas filas y columnas del conjunto de datos",) %>% kable_styling(latex_options = c("striped", "scale_down","hold_position"))
# Voy a descomponer en:
# - trend
# - seasonality
# - noise
#
# SMA(n):moving average of last n days --> smoothing
# decompose()
sapply(db,class)
sapply(db,class) %>% View()
library(tidyverse)
library(kableExtra)
library(ggpubr)
library(randomForest)
library(mlbench)
library(caret)
library(FactoMineR)
library(factoextra)
library(mltools)
library(data.table)
sapply(db,class) %>% View()
sapply(db,class) %>% as.data.frame() %>% View()
# Evaluamos los tipos de dato.
db_types <- sapply(db,class) %>% as.data.frame()
colnames(db_types) <- c("Data_types")
db_types %>% View()
db_types %>% slice(., 1:floor(nrow(db_types)/2)) %>% View()
# Evaluamos los tipos de dato.
db_types <- sapply(db,class) %>% as.data.frame()
colnames(db_types) <- c("Data_types")
db_types01 <- db_types %>% slice(., 1:floor(nrow(db_types)/2))
db_types02 <- db_types %>% slice(., floor(nrow(db_types)/2):nrow(db_types))
nrow(db_types)
nrow(db_types01)
nrow(db_types02)
db_types01 %>% View()
db_types02 %>% View()
?kbl
?kable
# Evaluamos los tipos de dato.
db_types <- sapply(db,class) %>% as.data.frame()
colnames(db_types) <- c("Data_types")
db_types01 <- db_types %>% slice(., 1:floor(nrow(db_types)/2))
db_types02 <- db_types %>% slice(., ceiling(nrow(db_types)/2):nrow(db_types))
kable(list(db_types01,db_types02), booktabs = T,caption = "Tipo de datos de las variables") %>% kable_styling(latex_options = c("striped", "scale_down","hold_position"))
db_types02 %>% View()
?knitr::kables()
tinytex::check_installed("booktabs")
print(db_types02 %>% kbl(., booktabs = T,caption = "Right") %>% kable_styling(latex_options = c("striped","hold_position")) ) %>% typeof()
?kbl
?kable_styling
include_graphics
?knitr::include_graphics
